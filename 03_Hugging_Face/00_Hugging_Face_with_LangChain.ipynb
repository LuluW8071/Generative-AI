{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVyR5rVU5VLW"
      },
      "source": [
        "# Hugging Face\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![ Hugging Face](https://media.licdn.com/dms/image/D5612AQH9oV7fFqFtYg/article-cover_image-shrink_720_1280/0/1707196998403?e=2147483647&v=beta&t=I1CfPU36DXToAl-9YkWTESiTnO9D7WgICP71PIBRtsk)\n",
        "</div>\n",
        "\n",
        "<img src=\"\">\n",
        "\n",
        "Hugging Face is a company and community known for its work in **natural language processing (NLP)** and **machine learning**. It provides a wide range of tools, including the popular **Transformers library**, which offers pre-trained models for tasks like **text classification**, **translation**, **summarization**, and **question-answering**. Hugging Face also supports other **machine learning models** and has an ecosystem that includes **datasets**, **model hubs**, and **development tools**. Their platform fosters **collaboration** and **accessibility** in the AI community, making advanced NLP technologies more widely available.\n",
        "\n",
        "\n",
        "## 00. Getting Started\n",
        "\n",
        "- __Activate virtual env (optional):__ To activate the virtual environment enter this your terminal:\n",
        "\n",
        "```bash\n",
        "      source env/bin/activate\n",
        "```\n",
        "\n",
        "- __Install Hugging Face Tools:__ In order to utilize hugging face tools, install the following packages:\n",
        "\n",
        "```bash\n",
        "      pip3 install huggingface-hub transformers accelerate bitsandbytes langchain-huggingface\n",
        "```\n",
        "\n",
        "- __Account Setup:__ First, create and [Hugging Face account](https://huggingface.co/join) or [log in](https://huggingface.co/login)\n",
        "\n",
        "- __API Key:__ After signing up, obtain your **Access Token** by navigating to the [`Settings>Access Tokens`](https://huggingface.co/settings/tokens) section.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Access Token](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new-token-dark.png)\n",
        "</div>\n",
        "\n",
        "> __Note:__ Token Access may require __READ__ and __WRITE__ permissions.\n",
        "\n",
        "- Next, create a `.env` file in the root directory of your project and add your Hugging Face Access Token key:\n",
        "\n",
        "```python\n",
        "HUGGINGFACEHUB_API_TOKEN =\"YOUR_TOKEN\"\n",
        "```\n",
        "\n",
        "> **Note:** Create a `.gitignore` file and add `.env` to it. This will ensure that your __API key__, __Tokens__ and other sensitive information in the `.env` file are not included in version control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lX1cQPay5VLa"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain import PromptTemplate, HuggingFaceHub\n",
        "\n",
        "# Load environment variables form .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access environment variables\n",
        "HUGGINGFACEHUB_API_TOKEN  = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJk4eehC5VLc"
      },
      "source": [
        "## 02. Overview of Hugging Face Models\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Model Overview](https://miro.medium.com/v2/resize:fit:1400/1*hi9dtGyJoe0q2GB33cS3ig.png)\n",
        "</div>\n",
        "\n",
        "Hugging Face offers a diverse range of models across various categories, including:\n",
        "\n",
        "- **Transformers**: State-of-the-art models like **BERT**, **GPT**, **T5**, and **RoBERTa** for tasks such as text classification, translation, summarization, and question-answering.\n",
        "\n",
        "- **Sequence-to-Sequence**: Models like **T5** and **BART** for text generation and translation.\n",
        "\n",
        "- **Token Classification**: Models for named entity recognition (NER) and other token-level tasks.\n",
        "\n",
        "- **Text Generation**: Models like **GPT-3** and **GPT-4** for generating coherent and contextually relevant text.\n",
        "\n",
        "- **Vision Transformers**: Models for image classification and object detection.\n",
        "\n",
        "- **Multimodal Models**: Models that integrate both text and image data for tasks like image captioning and visual question answering.\n",
        "\n",
        "These models are pre-trained and fine-tuned on various datasets, making them versatile for numerous NLP and machine learning applications.\n",
        "\n",
        "### 2.1 [Google Flan T5 Model Large [Encoder-Decoder]](https://huggingface.co/google/flan-t5-large)\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Google Flan T5 Model](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg)\n",
        "</div>\n",
        "\n",
        "- **Architecture**: Based on T5, handles all tasks as text-to-text (input and output are text).\n",
        "\n",
        "- **Pre-training**: Trained on diverse tasks, fine-tuned with instruction-based data for better task-specific understanding.\n",
        "\n",
        "- **Size**: \"Large\" version has ~770 million parameters; larger versions (e.g., XXL) have even more.\n",
        "\n",
        "- **Capabilities**: Excels at text completion, summarization, translation, and question-answering.\n",
        "\n",
        "- **Use Cases**: Suitable for complex text generation, multi-step reasoning, and tasks requiring detailed instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1bfrCps5VLd",
        "outputId": "874e65d6-3c2f-42f1-d9a2-ed23020be77e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Create a prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is good name for {product} that has amazing new features\"\n",
        ")\n",
        "\n",
        "# Initialize Hugging Face Model and set temperature\n",
        "model = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
        "                       model_kwargs={\"temperature\":1.5})\n",
        "\n",
        "# Create a chain\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly9deSpe-5kf",
        "outputId": "cf22063b-ac21-4953-ad2d-d2ff1a3c9305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "moto g\n",
            "dslr\n"
          ]
        }
      ],
      "source": [
        "# Generate response\n",
        "print(chain.invoke(\"smartphone\"))\n",
        "print(chain.invoke(\"camera\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeXVrywf_KCb"
      },
      "source": [
        "### 2.2 [Mistral 7B Instruct v0.2 [Decoder Only]](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Model Overview](https://miro.medium.com/v2/resize:fit:1400/1*7sBn0_bwT7_x5Fe3iegqvg.png)\n",
        "</div>\n",
        "\n",
        "\n",
        "- **Architecture**: Mistral-7B-Instruct is based on the Mistral architecture, designed for instruction-following tasks with a focus on understanding and generating text based on specific prompts.\n",
        "\n",
        "- **Pre-training**: Trained on diverse datasets to handle a range of instructions and tasks, enhancing its ability to follow complex directives.\n",
        "\n",
        "- **Size**: Contains 7 billion parameters, balancing high performance with manageable computational requirements.\n",
        "\n",
        "- **Capabilities**: Excels at following detailed instructions, text generation, and performing tasks based on specific user prompts.\n",
        "\n",
        "- **Use Cases**: Suitable for applications requiring precise instruction-following, detailed text generation, and scenarios where complex user inputs are involved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBCUGx1G-yFx",
        "outputId": "cccfd302-8b92-415e-e1e6-090627ddc246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "路路路路路路路路路路\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGvcvIbz57TQ",
        "outputId": "fb86b963-994b-4637-823a-3562f435386b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs = {\"max_length\":128},\n",
        "    temperature=0.5,\n",
        "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
        ")\n",
        "\n",
        "llm_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tza8gbHu_hfh",
        "outputId": "a185b53d-9afa-4027-9bd6-1c33b4c81df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "?\n",
            "\n",
            "Here are some suggestions for a camera with amazing new features:\n",
            "\n",
            "1. FuturaCam: This name suggests advanced technology and a forward-thinking design.\n",
            "2. VisionaryCam: This name emphasizes the camera's innovative capabilities and its ability to capture unique perspectives.\n",
            "3. PinnacleCam: This name conveys excellence and the idea that this camera is the best of the best.\n",
            "4. InnovizCam: This name highlights the camera's cutting-edge technology and its role as a leader in the industry.\n",
            "5. GeniusCam: This name suggests intelligence and the camera's ability to capture and process complex information.\n",
            "6. ProdigyCam: This name implies exceptional ability and a high level of performance.\n",
            "7. MarvelCam: This name suggests something amazing and worthy of wonder.\n",
            "8. MirageCam: This name suggests an illusion or a dreamlike quality, which could be an intriguing selling point for a camera with advanced features.\n",
            "9. NexusCam: This name suggests a connection or link between different things, which could be a good fit for a camera that brings together various advanced features.\n",
            "10. ApexCam: This name suggests the highest point or pinnacle, which could be a good fit for a camera that offers the best possible image quality and features.\n"
          ]
        }
      ],
      "source": [
        "print(llm_chain.invoke(\"camera\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byw4rKTJ5VLh",
        "outputId": "f3fa801f-4a4d-4176-8bd9-768f860479ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Create a prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"sentence\"],\n",
        "    template=\"{sentence}. Translate the sentence into spanish language.\"\n",
        ")\n",
        "\n",
        "# Create a model endpoint from hugging face and chain it\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs = {\"max_length\":64},\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
        ")\n",
        "\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu5D9XSE5VLi",
        "outputId": "cf25200f-fb46-42e8-97e7-5ce990c6ecbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "To translate the sentence into Spanish, you can use a translation tool or consult a bilingual speaker. Here's the translation:\n",
            "\n",
            "PyTorch es el mejor marco para construir modelos de aprendizaje profundo.\n",
            "\n",
            "This sentence means: PyTorch is the best framework for building deep learning models. in Spanish language.\n"
          ]
        }
      ],
      "source": [
        "# Generate a response\n",
        "print(chain.invoke(\"PyTorch is the best framework to build deep learning models\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK9z4uaOBG_z"
      },
      "source": [
        "> Before, we just ran the inference of tranformer models and LLMs through hugging face. Now lets locally download the __Google Flan-T5-Large__ model and run inference on local device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wv__IVqEAVrF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dZRjX8-nCSqH"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/flan-t5-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pugQeP1lDV53"
      },
      "outputs": [],
      "source": [
        "# Download required tokenizers and pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_tjMj9mjD3cL"
      },
      "outputs": [],
      "source": [
        "# Create a pipeline\n",
        "pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "local_llm = HuggingFacePipeline(pipeline=pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3xky1yDZE6AK"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"Tell me about footballer {name}\"\n",
        ")\n",
        "\n",
        "chain = prompt | local_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LytvcRXEIWuc",
        "outputId": "9afc86ba-aa48-4979-c1b6-a1e931c0cd59"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Maradona (born 4 April 1998) is a retired footballer from Argentina.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"Maradona\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4xzmzzmpIaAx",
        "outputId": "7b5c8350-e4d9-413c-bb18-2ec8b1fb17c1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Messi (born June 26, 1985) is a Spanish footballer who plays for FC Barcelona.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"Messi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp5aRJVBLXyy"
      },
      "source": [
        "> Looks like the **date of birth is wrong** as well  since **maradona played before messi was born**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6_ddFxY4KMqu",
        "outputId": "b6c06a20-30c5-4cb4-d71d-3d7d4145e22b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Stephen Hawking (born 1 July 1959) is an English mathematician.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lets trick the model by asking about other famous person while prompt being about footballer\n",
        "chain.invoke(\"Stephen Hawking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgUCdsMTKmZo"
      },
      "source": [
        "> This response is wrong though as we can see __t5 large model__ is not smart enough compared to LLMs \n",
        "\n",
        "> __Stephen Hawking__ (born 1 July 1942) was an English theoretical physicist, cosmologist, and author, known for his work on black holes and cosmology."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W_bDdInaKbx-"
      },
      "outputs": [],
      "source": [
        "# So lets change the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"Tell me about physicist and cosmologist {name}\"\n",
        ")\n",
        "\n",
        "chain = prompt | local_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wfwHfcaKK_fV",
        "outputId": "96c0f5b3-13f4-4f88-e220-ac86dd5518d3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Stephen Hawking (born 1 April 1959) is a British physicist and cosmologist.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"Stephen Hawking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwD338hVL4dt"
      },
      "source": [
        "## Why Check Model Responses and Data Alignment ??? \n",
        "\n",
        "- **Read Papers**\n",
        "- **Verify Training Data Relevance**\n",
        "- **Evaluate Inference Quality and Metrics**\n",
        "- **Check Against Reference Responses**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
