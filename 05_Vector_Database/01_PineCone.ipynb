{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc3f7e5",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/sites/2148158644/images/85d1d5-f44-11de-03a0-b8b70657ae6f_pinecone.jpeg\" width=80%>\n",
    "</center>\n",
    "\n",
    "**[Pinecone](https://www.pinecone.io/)** is a fully managed vector database built specifically for **high-performance vector search**. It helps you **store, index, and search vector embeddings** at scale—without managing the infrastructure yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2834e06",
   "metadata": {},
   "source": [
    "To use **Pinecone** for semantic vector storage and search, we’ll need:\n",
    "\n",
    "* `langchain`: chaining and embedding integration\n",
    "\n",
    "* `pinecone-client`: interact with **Pinecone DB**\n",
    "\n",
    "* Either:\n",
    "    * `openai` + `tiktoken`: for **OpenAI embeddings** or\n",
    "    * `google-generativeai`: for **Gemini embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8ee023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (using gemini embeddings)\n",
    "!pip install langchain pinecone-client pypdf -q\n",
    "!pip install \"google-ai-generativelanguage>=0.6.18,<0.7.0\" langchain-google-genai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431665a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97c4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../assets/MachineTranslationwithAttention.pdf\")\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9648ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.21',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2021-11-05T20:59:50+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2021-11-05T20:59:50+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241',\n",
       " 'source': '../assets/MachineTranslationwithAttention.pdf',\n",
       " 'total_pages': 11,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91af779d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neural Machine Translation with Attention\\nMohammad Wasil Saleem\\nMatrikel-Nr.: 805779\\nUniversit¨at Potsdam\\nsaleem1@uni-potsdam.de\\nSandeep Uprety\\nMatrikel-Nr. 804982\\nUniversit¨at Potsdam\\nuprety@uni-potsdam.de\\nAbstract\\nIn recent years, the success achieved\\nthrough neural machine translation has\\nmade it mainstream in machine translation\\nsystems. In this work, encoder-decoder\\nwith attention system based on ”Neural\\nMachine Translation by Jointly Learning\\nto Align and Translate” by Bahdanau et al.\\n(2014) has been used to accomplish the\\nMachine Translation between English and\\nSpanish Language which has not seen\\nmuch research work done as compared\\nto other languages such as German and\\nFrench. We aim to demonstrate the re-\\nsults similar to the breakthrough paper on\\nwhich our work is based on. We achieved\\na BLEU score of 25.37, which was close\\nenough to what Bahdanau et al. (2014)\\nachieved in their work.\\n1 Introduction\\nMachine Translation (MT) is the task of translat-\\ning text without human assistance while preserv-\\ning the meaning of input text. The early approach\\nto machine translation relied heavily on hand-\\ncrafted translation rules and linguistic knowledge.\\nStarted in early around 1950s, unlike rule-based\\nmachine translation, Statistical machine transla-\\ntion (SMT) generated translations based on statis-\\ntical models whose parameters are derived from\\nthe analysis of bilingual text corpora (Koehn et al.,\\n2003). Though reliable, for SMT, it can be hard\\nto ﬁnd content for obscure languages and is less\\nsuitable for language pairs with big differences\\nin word order making the quality of translation\\nfar from satisfactory. With the progress in deep\\nlearning being applied to MT, in 2014, end-to-end\\nneural network translation model was proposed\\nby (Bahdanau et al., 2014; Sutskever et al., 2014)\\nwhere the term ”neural machine translation” was\\nformally used. Neural machine translation (NMT)\\nis the newest method of MT that uses a single\\nlarge neural network to model the entire transla-\\ntion process, freeing the need for excessive fea-\\nture engineering. Through the rapid research and\\nbreakthroughs, end-to-end neural machine trans-\\nlation has gained remarkable performances (Shi\\net al., 2021; Bahdanau et al., 2014) and have be-\\ncome mainstream approach to MT.\\n2 Related Work\\nEarly problem of NMT was often the poor trans-\\nlation for long sentences (Sutskever et al., 2014)\\nwhich can be attributed to the ﬁxed-length of\\nsource encoding in conventional encoder-decoder\\nas suggested by Cho et al. (2014a) for which the\\nconcept of attention to NMT was introduced by\\nBahdanau et al. (2014) to avoid keeping a ﬁxed\\nsource side representation.\\nAs compared to separately tuned components in\\nSMT, newly emerging Neural Machine translation\\nradically departures from previous machine learn-\\ning approaches as the training of NMT is end-to-\\nend which has signiﬁcantly improved translation\\nquality across 30 different languages (Junczys-\\nDowmunt et al., 2016). NMT model can be attrac-\\ntive for various reason one being scalability issue,\\nwhether it be memory requirements or computa-\\ntional speed. Another being able to train all the\\ncharacter embedding as each characters frequently\\noccurs in the training corpus.\\nMost neural machine translation models pro-\\nposed use encoder-decoder where a neural net-\\nwork reads and encodes a source sentence into\\na ﬁxed-length vector and a decoder then outputs\\na translation from the encoded vector, where in\\nmost of the cases the encoder and decoder are\\nmainly implemented as RNNs, CNNs or self-\\nattention network (Wu et al., 2018). The whole\\nencoder–decoder system, which consists of the\\nencoder and the decoder for a language pair, is'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad257296",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92eff667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/355917108\\nNeural Machine Translation with Attention\\nTechnical Report · August 2021\\nDOI: 10.13140/RG.2.2.29381.37607/1\\nCITATION\\n1\\nREADS\\n5,448\\n2 authors:\\nMohammad Wasil Saleem\\nUniversität Potsdam\\n4 PUBLICATIONS\\xa0\\xa0\\xa02 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nSandeep Uprety\\nUniversität Potsdam\\n1 PUBLICATION\\xa0\\xa0\\xa01 CITATION\\xa0\\xa0\\xa0\\nSEE PROFILE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='SEE PROFILE\\nAll content following this page was uploaded by Sandeep Uprety on 05 November 2021.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Neural Machine Translation with Attention\\nMohammad Wasil Saleem\\nMatrikel-Nr.: 805779\\nUniversit¨at Potsdam\\nsaleem1@uni-potsdam.de\\nSandeep Uprety\\nMatrikel-Nr. 804982\\nUniversit¨at Potsdam\\nuprety@uni-potsdam.de\\nAbstract\\nIn recent years, the success achieved\\nthrough neural machine translation has\\nmade it mainstream in machine translation\\nsystems. In this work, encoder-decoder\\nwith attention system based on ”Neural\\nMachine Translation by Jointly Learning\\nto Align and Translate” by Bahdanau et al.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = text_splitter.split_documents(pages)\n",
    "text_chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6b1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2014) has been used to accomplish the\n",
      "Machine Translation between English and\n",
      "Spanish Language which has not seen\n",
      "much research work done as compared\n",
      "to other languages such as German and\n",
      "French. We aim to demonstrate the re-\n",
      "sults similar to the breakthrough paper on\n",
      "which our work is based on. We achieved\n",
      "a BLEU score of 25.37, which was close\n",
      "enough to what Bahdanau et al. (2014)\n",
      "achieved in their work.\n",
      "1 Introduction\n",
      "Machine Translation (MT) is the task of translat-\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b215e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing text without human assistance while preserv-\n",
      "ing the meaning of input text. The early approach\n",
      "to machine translation relied heavily on hand-\n",
      "crafted translation rules and linguistic knowledge.\n",
      "Started in early around 1950s, unlike rule-based\n",
      "machine translation, Statistical machine transla-\n",
      "tion (SMT) generated translations based on statis-\n",
      "tical models whose parameters are derived from\n",
      "the analysis of bilingual text corpora (Koehn et al.,\n",
      "2003). Though reliable, for SMT, it can be hard\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[4].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e2f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66e30783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.014134909026324749,\n",
       " -0.022324152290821075,\n",
       " -0.054603420197963715,\n",
       " -0.006284549366682768,\n",
       " -0.03392402455210686]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a248576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)     # Dimension of the embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f310716",
   "metadata": {},
   "source": [
    "pip install langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone.vectorstores import Pinecone as PC\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"generative-ai\"\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991c7de",
   "metadata": {},
   "source": [
    "### Create Embeddings for each of the Text Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75c9bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = PC.from_texts(\n",
    "    [t.page_content for t in text_chunks],\n",
    "    embeddings,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d89a1c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.Pinecone at 0x79ad18329610>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4e4acb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3b2a3d55-1d6e-4901-8e8f-9bb13f87648d', metadata={}, page_content='coder output and all of the encoder hidden state.\\nWe need to learn this alignment. Each output of\\nthe decoder can selectively pick out speciﬁc ele-\\nments from the sequence to produce the output.\\nSo, this allows the model to focus and pay more\\n”Attention” to the relevant part of the input se-\\nquence.\\nThe ﬁrst attention model was proposed by Bah-\\ndanau et al. (2014), there are several other types of\\nattention proposed, such as the one by Luong et al.'),\n",
       " Document(id='58860f51-d46d-4cf9-a85a-3fc324e8149c', metadata={}, page_content='Figure 2: Attention Model (Luong et al., 2015)\\nuse the information from the past, present as well\\nas from the future. We need the entire sequence of\\ndata before we can make any predictions.\\nThe architecture that we are proposing here is\\nbased on the Encoder-Decoder Framework. The\\nencoder takes in the input sentence and converts\\nthem into a vector representation\\nThe encoder can be an RNN (Cho et al., 2014b)\\nor LSTM unit (Sutskever et al., 2014). They pro-'),\n",
       " Document(id='ca92ef4b-7864-438b-ab87-f5c23a507d5b', metadata={}, page_content='to summarise large input sequence at once. This\\nalso poses a problem where the encoder is not able\\nto memorize the words coming at the beginning\\nof the sentences, which leads to poor translation\\nof the source sentence. The Attention mechanism\\njust addresses this issue, by retaining and utilising\\nall the hidden state of the input sentence during the\\ndecoding phase.\\nDuring the decoding phase, the model creates\\nan alignment between each time step of the de-'),\n",
       " Document(id='140d06c9-98d6-4c43-8b33-9561fa5921c6', metadata={}, page_content='Even though if we used LSTM or GRU, which\\ntends to remember the words that occured very\\nearly in the sequence, it will still not be able to\\nlearn the alignment between the source word and\\nthe target word. They often forget the initial part\\nof the sentence once they are processed in the en-\\ncoder. That is why we use an alignment mecha-\\nnism called attention. They help to memorize this\\ninformation for longer sentences. But we need to\\nlearn this alignment. It can vary from language to\\nlanguage.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"What is attention mechanism?\"\n",
    "\n",
    "docs = docsearch.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23b30306",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30f1a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a4e1637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is attention mechanism?',\n",
       " 'result': 'The attention mechanism addresses the issue of the encoder not being able to memorize words at the beginning of sentences, which leads to poor translation. It retains and utilizes all the hidden states of the input sentence during the decoding phase, creating an alignment between each time step of the decoder output and the encoder hidden state. This allows the model to focus and pay more attention to the relevant parts of the input sequence.'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd4c9df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The attention mechanism can improve the translation task because the encoder-decoder model with attention mechanism cope better with long sentences.\\n\\nThe BLEU score reported by Bahdanau et al. (2014) when using the attention mechanism was 26.75, training with 1000 encoder and decoder dimensions, and training on corpus of 384M words. The text mentions that the model without attention mechanism was underperforming with long sentences, but it does not specify the BLEU score.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query = \"\"\"\n",
    "\"How can attention mechanism improve the translation task?\n",
    "What BLEU score was reported when using and not using the attention mechanism\n",
    "\"\"\"\n",
    "\n",
    "response = qa.invoke(new_query)\n",
    "response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc78202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
