{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc3f7e5",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/sites/2148158644/images/85d1d5-f44-11de-03a0-b8b70657ae6f_pinecone.jpeg\" width=80%>\n",
    "</center>\n",
    "\n",
    "**[Pinecone](https://www.pinecone.io/)** is a fully managed vector database built specifically for **high-performance vector search**. It helps you **store, index, and search vector embeddings** at scale—without managing the infrastructure yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2834e06",
   "metadata": {},
   "source": [
    "To use **Pinecone** for semantic vector storage and search, we’ll need:\n",
    "\n",
    "* `langchain`: chaining and embedding integration\n",
    "\n",
    "* `pinecone-client`: interact with **Pinecone DB**\n",
    "\n",
    "* Either:\n",
    "    * `openai` + `tiktoken`: for **OpenAI embeddings** or\n",
    "    * `google-generativeai`: for **Gemini embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8ee023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (using gemini embeddings)\n",
    "!pip install langchain pinecone-client pypdf -q\n",
    "!pip install \"google-ai-generativelanguage>=0.6.18,<0.7.0\" langchain-google-genai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431665a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee58ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x7f4f5a657440>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"../assets/MachineTranslationwithAttention.pdf\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e97c4b89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m data = loader.load()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "data = loader.load()\n",
    "data.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9648ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Machine Translation with Attention\n",
      "Mohammad Wasil Saleem\n",
      "Matrikel-Nr.: 805779\n",
      "Universit¨at Potsdam\n",
      "saleem1@uni-potsdam.de\n",
      "Sandeep Uprety\n",
      "Matrikel-Nr. 804982\n",
      "Universit¨at Potsdam\n",
      "uprety@uni-potsdam.de\n",
      "Abstract\n",
      "In recent years, the success achieved\n",
      "through neural machine translation has\n",
      "made it mainstream in machine translation\n",
      "systems. In this work, encoder-decoder\n",
      "with attention system based on ”Neural\n",
      "Machine Translation by Jointly Learning\n",
      "to Align and Translate” by Bahdanau et al.\n",
      "(2014) has been used to accomplish the\n",
      "Machine Translation between English and\n",
      "Spanish Language which has not seen\n",
      "much research work done as compared\n",
      "to other languages such as German and\n",
      "French. We aim to demonstrate the re-\n",
      "sults similar to the breakthrough paper on\n",
      "which our work is based on. We achieved\n",
      "a BLEU score of 25.37, which was close\n",
      "enough to what Bahdanau et al. (2014)\n",
      "achieved in their work.\n",
      "1 Introduction\n",
      "Machine Translation (MT) is the task of translat-\n",
      "ing text without human assistance while preserv-\n",
      "ing the meaning of input text. The early approach\n",
      "to machine translation relied heavily on hand-\n",
      "crafted translation rules and linguistic knowledge.\n",
      "Started in early around 1950s, unlike rule-based\n",
      "machine translation, Statistical machine transla-\n",
      "tion (SMT) generated translations based on statis-\n",
      "tical models whose parameters are derived from\n",
      "the analysis of bilingual text corpora (Koehn et al.,\n",
      "2003). Though reliable, for SMT, it can be hard\n",
      "to ﬁnd content for obscure languages and is less\n",
      "suitable for language pairs with big differences\n",
      "in word order making the quality of translation\n",
      "far from satisfactory. With the progress in deep\n",
      "learning being applied to MT, in 2014, end-to-end\n",
      "neural network translation model was proposed\n",
      "by (Bahdanau et al., 2014; Sutskever et al., 2014)\n",
      "where the term ”neural machine translation” was\n",
      "formally used. Neural machine translation (NMT)\n",
      "is the newest method of MT that uses a single\n",
      "large neural network to model the entire transla-\n",
      "tion process, freeing the need for excessive fea-\n",
      "ture engineering. Through the rapid research and\n",
      "breakthroughs, end-to-end neural machine trans-\n",
      "lation has gained remarkable performances (Shi\n",
      "et al., 2021; Bahdanau et al., 2014) and have be-\n",
      "come mainstream approach to MT.\n",
      "2 Related Work\n",
      "Early problem of NMT was often the poor trans-\n",
      "lation for long sentences (Sutskever et al., 2014)\n",
      "which can be attributed to the ﬁxed-length of\n",
      "source encoding in conventional encoder-decoder\n",
      "as suggested by Cho et al. (2014a) for which the\n",
      "concept of attention to NMT was introduced by\n",
      "Bahdanau et al. (2014) to avoid keeping a ﬁxed\n",
      "source side representation.\n",
      "As compared to separately tuned components in\n",
      "SMT, newly emerging Neural Machine translation\n",
      "radically departures from previous machine learn-\n",
      "ing approaches as the training of NMT is end-to-\n",
      "end which has signiﬁcantly improved translation\n",
      "quality across 30 different languages (Junczys-\n",
      "Dowmunt et al., 2016). NMT model can be attrac-\n",
      "tive for various reason one being scalability issue,\n",
      "whether it be memory requirements or computa-\n",
      "tional speed. Another being able to train all the\n",
      "character embedding as each characters frequently\n",
      "occurs in the training corpus.\n",
      "Most neural machine translation models pro-\n",
      "posed use encoder-decoder where a neural net-\n",
      "work reads and encodes a source sentence into\n",
      "a ﬁxed-length vector and a decoder then outputs\n",
      "a translation from the encoded vector, where in\n",
      "most of the cases the encoder and decoder are\n",
      "mainly implemented as RNNs, CNNs or self-\n",
      "attention network (Wu et al., 2018). The whole\n",
      "encoder–decoder system, which consists of the\n",
      "encoder and the decoder for a language pair, is\n"
     ]
    }
   ],
   "source": [
    "print(data[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad257296",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92eff667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/355917108\\nNeural Machine Translation with Attention\\nTechnical Report · August 2021\\nDOI: 10.13140/RG.2.2.29381.37607/1\\nCITATION\\n1\\nREADS\\n5,448\\n2 authors:\\nMohammad Wasil Saleem\\nUniversität Potsdam\\n4 PUBLICATIONS\\xa0\\xa0\\xa02 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nSandeep Uprety\\nUniversität Potsdam\\n1 PUBLICATION\\xa0\\xa0\\xa01 CITATION\\xa0\\xa0\\xa0\\nSEE PROFILE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='SEE PROFILE\\nAll content following this page was uploaded by Sandeep Uprety on 05 November 2021.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Neural Machine Translation with Attention\\nMohammad Wasil Saleem\\nMatrikel-Nr.: 805779\\nUniversit¨at Potsdam\\nsaleem1@uni-potsdam.de\\nSandeep Uprety\\nMatrikel-Nr. 804982\\nUniversit¨at Potsdam\\nuprety@uni-potsdam.de\\nAbstract\\nIn recent years, the success achieved\\nthrough neural machine translation has\\nmade it mainstream in machine translation\\nsystems. In this work, encoder-decoder\\nwith attention system based on ”Neural\\nMachine Translation by Jointly Learning\\nto Align and Translate” by Bahdanau et al.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='(2014) has been used to accomplish the\\nMachine Translation between English and\\nSpanish Language which has not seen\\nmuch research work done as compared\\nto other languages such as German and\\nFrench. We aim to demonstrate the re-\\nsults similar to the breakthrough paper on\\nwhich our work is based on. We achieved\\na BLEU score of 25.37, which was close\\nenough to what Bahdanau et al. (2014)\\nachieved in their work.\\n1 Introduction\\nMachine Translation (MT) is the task of translat-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='ing text without human assistance while preserv-\\ning the meaning of input text. The early approach\\nto machine translation relied heavily on hand-\\ncrafted translation rules and linguistic knowledge.\\nStarted in early around 1950s, unlike rule-based\\nmachine translation, Statistical machine transla-\\ntion (SMT) generated translations based on statis-\\ntical models whose parameters are derived from\\nthe analysis of bilingual text corpora (Koehn et al.,\\n2003). Though reliable, for SMT, it can be hard'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='to ﬁnd content for obscure languages and is less\\nsuitable for language pairs with big differences\\nin word order making the quality of translation\\nfar from satisfactory. With the progress in deep\\nlearning being applied to MT, in 2014, end-to-end\\nneural network translation model was proposed\\nby (Bahdanau et al., 2014; Sutskever et al., 2014)\\nwhere the term ”neural machine translation” was\\nformally used. Neural machine translation (NMT)\\nis the newest method of MT that uses a single'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='large neural network to model the entire transla-\\ntion process, freeing the need for excessive fea-\\nture engineering. Through the rapid research and\\nbreakthroughs, end-to-end neural machine trans-\\nlation has gained remarkable performances (Shi\\net al., 2021; Bahdanau et al., 2014) and have be-\\ncome mainstream approach to MT.\\n2 Related Work\\nEarly problem of NMT was often the poor trans-\\nlation for long sentences (Sutskever et al., 2014)\\nwhich can be attributed to the ﬁxed-length of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='source encoding in conventional encoder-decoder\\nas suggested by Cho et al. (2014a) for which the\\nconcept of attention to NMT was introduced by\\nBahdanau et al. (2014) to avoid keeping a ﬁxed\\nsource side representation.\\nAs compared to separately tuned components in\\nSMT, newly emerging Neural Machine translation\\nradically departures from previous machine learn-\\ning approaches as the training of NMT is end-to-\\nend which has signiﬁcantly improved translation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='quality across 30 different languages (Junczys-\\nDowmunt et al., 2016). NMT model can be attrac-\\ntive for various reason one being scalability issue,\\nwhether it be memory requirements or computa-\\ntional speed. Another being able to train all the\\ncharacter embedding as each characters frequently\\noccurs in the training corpus.\\nMost neural machine translation models pro-\\nposed use encoder-decoder where a neural net-\\nwork reads and encodes a source sentence into'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='a ﬁxed-length vector and a decoder then outputs\\na translation from the encoded vector, where in\\nmost of the cases the encoder and decoder are\\nmainly implemented as RNNs, CNNs or self-\\nattention network (Wu et al., 2018). The whole\\nencoder–decoder system, which consists of the\\nencoder and the decoder for a language pair, is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='jointly trained to maximize the probability of a\\ncorrect translation given a source sentence (Bah-\\ndanau et al., 2014). After the initial proposal by\\n(Sutskever et al., 2014; Bahdanau et al., 2014),\\nmuch work has been done on the sequence-to-\\nsequence neural machine translation model rang-\\ning from new attention mechanism (Luong et al.,\\n2015) to working on the problem of out-of-\\nvocabulary words (Jean et al., 2015) for which se-\\nquential RNNs are used both for encoding source'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='sentences and generating target translation.\\nWe draw our inspiration for machine transla-\\ntion with attention from Bahdanau et al. (2014).\\nWe have chosen to base our project on this paper\\nas attention mechanism has been widely used as\\nbaseline and is thoroughly studied among the NLP\\ncommunity.\\n3 Model\\nMachine Translation is equivalent to maximizing a\\nconditional probability of a target sentence given\\nthe source sentence. In Neural Machine Trans-\\nlation, we parameterize it to maximise the condi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='tional probability. The approach used by Cho et al.\\n(2014b) was to encode the source sentence into\\na ﬁxed-length vector, which becomes difﬁcult to\\ncompress all the necessary information into a ﬁxed\\nlength vector, which makes it difﬁcult for the Neu-\\nral Network to handle long sentences, and thus the\\nperformance of the encoder-decoder drops as the\\nlength of the sentences increases. So we use the\\nmodel proposed by Bahdanau et al. (2014), where\\nit does not encode the input sentence into a ﬁxed-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='length vector, rather than it simply encodes the in-\\nput sentence into sequence of vectors, and while\\ndecoding the translation, it select subset of vectors\\nfrom the using attention mechanism. And Bah-\\ndanau et al. (2014) showed that encoder decoder\\nmodel with attention mechanism cope better with\\nlong sentences. In the next section, we will ﬁrst\\ngive a brief introduction on encoder-decoder, the\\nRNN, and one of its type, GRU, the one we used\\nin our model and ﬁnally the attention mechanism.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='3.1 Encoder-Decoder\\nEncoder-Decoder, ﬁrst proposed by Cho et al.\\n(2014b), basically consists of 2 parts, the encoder\\nand the decoder. Encoder codes the sequence\\nof input sentence into dense vector representa-\\ntion, and then decoder takes in the encoded sen-\\ntence and decode the representation into another\\nsequences of words. They are trained to maximize\\nthe conditional probability of the output sentence,\\ngiven the input sentence.\\nRNN is necessary when we need to maintain the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='word order in a sentence. This is not handled by\\nbags of words model or other statistical models. In\\naddition to input, xi and output ˆyi, we also have a\\nstate vector, ai, which is initialized with vectors of\\nzeros. iwould be the ith timestep In the ﬁrst layer\\nof RNN, the input and state vector is fed into the\\nrecurrent unit, the recurent unit may look like 1:\\nat = f(Waaat−1 + Waxxi),\\nand,\\nˆyt = g(Wyaat)\\nwhere, t is the time step, Waa is the weights be-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='tween two hidden layer, Wax is the weight be-\\ntween input and hidden layer, and Wya is the\\nweight between hidden and the output layer, and\\nf can be tanhor Reluactivation function, and g\\ncan be sigmoid or softmax activation function.\\nAfter feeding the input to rnn unit, it returns a new\\nstate vector in the next time step. This new state\\nvector will be mapped to the output vector using\\nsome function. This output vector can be used as\\na prediction. The new state vector is cached and is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='passed across the next unit of the RNN, along with\\nthe input in order for it to return the next state vec-\\ntor. This happens recursively for all the input ele-\\nments. So, when the model is reading the second\\nword, instead of just predicting output using only\\nthe second word, it also gets information from the\\nprevious time step (ﬁrst word) in terms of the state\\nvector.\\nOne of the problem of RNN is that it runs into\\nthe problem of Vaishing gradient, ﬁrst described'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='by Hochreiter (1998). This happens when we have\\na very long sentence, which tends to have long\\nterm dependencies. That means a word at the end\\nof the sentence would be semantically dependent\\non the word occurring at the beginning of the sen-\\ntence. So, the gradients during the backpropaga-\\ntion step would have a very hard time propagating\\nback to affect the words or weights of the earlier\\nunits. The gradients diminishes in the backpropa-\\ngation step and not able to reach the earlier units.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='Generally, RNN has local inﬂuences where a word\\nis mainly inﬂuenced by words closed to it. So that\\nmakes it difﬁcult for the output at the later unit\\n1https://www.coursera.org/learn/nlp-sequence-\\nmodels/home/welcome'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='Figure 1: The left diagram represents Long Short Term Memory Unit, with ias an Input gate, oas an\\nOutput Gate, and f as a Forget Gate. The right diagram represents Gated Recurrent Unit, withras Reset\\nGate, and zas an Update Gate. (Chung et al., 2014)\\nto be strongly inﬂuenced by a word that was very\\nearly in the sequence.\\nBut we can further improve the training by us-\\ning other RNN units, like GRU (Chung et al.,\\n2014) or LSTM (Hochreiter and Schmidhuber,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='1997), which are better at capturing long-range de-\\npendencies. The state vector in simple RNN can\\nbe considered as a memory, where the memory ac-\\ncess was not controlled. At each step, the entire\\nmemory state was read and updated. But in GRU\\nand LSTM, we use a gating mechanism to control\\nthe memory. Since, we used GRU in our model,\\nso we will only describe GRU here.\\nIn GRU, (Chung et al., 2014; Rana, 2016), the\\nactivation hj\\nt is a linear interpolation between the\\nprevious activation hj'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='t−1 and the candidate ativa-\\ntion ˜hj\\nt :\\nhj\\nt = (1−zj\\nt )hj\\nt−1 + zj\\nt ˜hj\\nt\\nwhere, zj\\nt is the update gate, that decides how\\nmuch GRU units updates its activation [See Fig.\\n1]. The update gate is given by :\\nzj\\nt = σ(Wzxt + Uzht−1)j.\\nAnd the candidate activation ˆhj\\nt is computed by :\\nˆhj\\nt = tanh(Wxt + U(rt ⊙ht−1))j,\\nwhere ⊙denotes element-wise multiplication and\\nrj\\nt are reset gates. When a reset gate at speciﬁc\\ntime, t is set to 0, i.e. rj\\nt == 0, which makes the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='GRU to forget the past, i.e. forget the previous\\nstate vectors. This is considered same as reading\\nthe ﬁrst word of the input sentence. And ﬁnally,\\nwe can compute the reset gate by :\\nrj\\nt = σ(Wrxt + Urht−1)j.\\nOne of the weakness of RNN is that it only uses\\ninformation that is earlier in the sequence to make\\npredictions but not the information which are later\\nin the sequence. When predicting the output at\\ntime step i, it does not use the word at time step i+1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='or i+2 or any other words in the later time step. So,\\nit would be useful to know not just the information\\nfrom the words from the previous time step but\\nalso the information from the later time steps.\\nSo, we use Bidirectional RNN, ﬁrst proposed\\nby (Schuster and Paliwal, 1997). From a point\\nin time, it takes information from both the ear-\\nlier and later time step in the sequence. The\\nFirst RNN, which we called forward RNN, − →f is\\nfed the input sequence as it is. And the second'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='RNN, which is also called backward RNN, ← −f\\nis fed the input sequence in reverse order. This\\ngives two separate state vectors – a forward state\\nvector, − →hT\\nj , and a backward state vector ← −hT\\nj .\\n− →hT\\nj would be a sequence of forward hidden state\\nvectors, (− →h1,..., − →hTx ), and similarly, backward\\nstate vector ← −hT\\nj would be a sequence of backward\\nhidden state vector, (← −h1,..., ← −hTx ). And the out-\\nput at a speciﬁc timestep is accounted by the con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='catenation of output of two RNN’s, concatenating− →hj and ← −hj, i.e. hj = [− →hT\\nj ,← −hT\\nj ]. So, when pre-\\ndicting the output at a speciﬁc time step, it will'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='Figure 2: Attention Model (Luong et al., 2015)\\nuse the information from the past, present as well\\nas from the future. We need the entire sequence of\\ndata before we can make any predictions.\\nThe architecture that we are proposing here is\\nbased on the Encoder-Decoder Framework. The\\nencoder takes in the input sentence and converts\\nthem into a vector representation\\nThe encoder can be an RNN (Cho et al., 2014b)\\nor LSTM unit (Sutskever et al., 2014). They pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='cesses the input sentence, pass it through RNN or\\nLSTM, and when it encounters the end of sen-\\ntence, then the hidden state, that captured all the\\nrelevant information passes it to the decoder. Then\\nthis information is used to predict the translations\\nin the decoder, which can be RNN or LSTM, until\\nit predicts the end of the sentence token. The hid-\\nden state needs to remember every word from the\\ninput sentence. So that is why this model tends to\\nwork for short sentences and not long sentences.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='Even though if we used LSTM or GRU, which\\ntends to remember the words that occured very\\nearly in the sequence, it will still not be able to\\nlearn the alignment between the source word and\\nthe target word. They often forget the initial part\\nof the sentence once they are processed in the en-\\ncoder. That is why we use an alignment mecha-\\nnism called attention. They help to memorize this\\ninformation for longer sentences. But we need to\\nlearn this alignment. It can vary from language to\\nlanguage.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='language.\\n3.2 Attention\\nIn this section, we will speciﬁcally deﬁne the\\nalignment mechanism, [See Figure 2] that we used\\nin our model. In RNN Encoder-Decoder model\\n(Sutskever et al., 2014), we faced with the bottle-\\nneck problem, where the complete sequence of in-\\nformation of the source sentence, must be captured\\nby one single vector, i.e. the last hidden unit of the\\nencoder RNN is used as a context vector for the\\ndecoder, which becomes difﬁcult for the decoder'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='to summarise large input sequence at once. This\\nalso poses a problem where the encoder is not able\\nto memorize the words coming at the beginning\\nof the sentences, which leads to poor translation\\nof the source sentence. The Attention mechanism\\njust addresses this issue, by retaining and utilising\\nall the hidden state of the input sentence during the\\ndecoding phase.\\nDuring the decoding phase, the model creates\\nan alignment between each time step of the de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='coder output and all of the encoder hidden state.\\nWe need to learn this alignment. Each output of\\nthe decoder can selectively pick out speciﬁc ele-\\nments from the sequence to produce the output.\\nSo, this allows the model to focus and pay more\\n”Attention” to the relevant part of the input se-\\nquence.\\nThe ﬁrst attention model was proposed by Bah-\\ndanau et al. (2014), there are several other types of\\nattention proposed, such as the one by Luong et al.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Spanish en la estrategia 2020 , reconocimos el hecho de que , si queremos mantener nuestro\\nnivel de prosperidad en europa , tenemos que aumentar nuestra productividad .\\nEnglish in the 2020 strategy , we acknowledged the fact that , if we are to maintain our level\\nof prosperity in europe , we need to increase our productivity .\\nSpanish sin embargo , es algo que debemos hacer si queremos demostrar a los estados unidos'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='que nos deben considerar como un socio serio en la alianza contra el terrorismo .\\nEnglish yet do it we must , if we are to demonstrate to the usa that we are to be taken\\nseriously as a partner in an alliance against terrorism .\\nSpanish sabr´an ustedes que fue tambi´en a instancias de esta c´amara que la comisi´on entabl´o\\nnegociaciones , y estas han dado un resultado encomiable .\\nEnglish you will be aware that it was not least at the insistence of this house that the com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='mission entered into negotiations , and these have produced a creditable result .\\nTable 1: Examples of Spanish and English sentences from the dataset\\n(2015).\\nWe will only discuss the attention model, pro-\\nposed by Bahdanau et al. (2014). After the in-\\nput sequence is passed through the encoder, it pro-\\nduces hidden state for each of the elements in the\\nsequence (h1,...,h Tx ). Then we multiply the de-\\ncoders hidden state at time step t (s1,...,s Ty ),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='with all of the encoders hidden state, which gives\\nus the alignment score of each of the encoder out-\\nput with respect to the decoder input and hidden\\nstate at that time step:\\net = [sT\\nt h1,...,s T\\nt hTx ]\\nThe alignment score quantiﬁes the amount of\\nAttention the decoder will place on each of the en-\\ncoder outputs when producing the next output, so\\ninstead of looking at the entire sequence, it just\\nconcentrate on few relevant parts of the sequence\\nwhen predicting the next word.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='After calculating the alignment score, we pass\\nthe vector et through the softmax layer, to calcu-\\nlate the probability distribution.\\nαt = softmax(et)\\nThen we multiply each of the attention weights\\nwith each of the encoder hidden state, to get con-\\ntext vector, at\\nat =\\nTx∑\\ni=1\\nαt\\nihi\\nIf the attention score of speciﬁc element of the\\ninput sequence is close to 1, then its inﬂuence on\\nthe decoder output at that speciﬁc time step in-\\ncreases. And then ﬁnally, the context vector at'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='produced will be concatenated with the decoder\\nhidden state, st, i.e.\\n˜ht = [at,st]\\nand is fed into decoder RNN, which produces new\\nhidden state.\\n4 Data\\nWe used a Parallel Corpus for English-Spanish\\nlanguage, which was extracted from the proceed-\\nings of the European Parliament, also called Eu-\\nroparl dataset (Koehn, 2005). It contains 1.96\\nMillion sentences, each for English and Spanish.\\nMost common words and count for both languages\\nare shown below:\\nWords Count\\nde 1799827\\n, 1456229\\nla 1222089'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='la 1222089\\nque 992176\\n. 867284\\nen 790382\\nel 696521\\ny 692640\\na 577052\\nlos 548495\\nTable 2: Spanish\\nWords Count\\nthe 1956558\\n, 1371506\\nof 932044\\nto 875415\\n. 864674\\nand 747108\\nin 622426\\nthat 476250\\na 430093\\nis 401782\\nTable 3: English\\nFirst we ﬁlter out all the sentences having words\\ngreater than 50. Then we sort these sentences\\nbased on the number of words in each of the sen-\\ntences, so that we have less padded sentences in\\nour initial indices and sentences with high padding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='to be at the end of our indices, following with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='usual tokenization methods. Only preprocessing\\nwe used was to lower case the words.\\nWe selected 1 million sentences for the model-\\ning due to the hardware constraints. We split the\\ndataset into usual format, i.e. train, test and vali-\\ndation. We used 900K sentences for our training,\\n80K for validation and remaining 20K sentences\\nfor the test set, which was not seen by the model\\nduring training. We did not limit the vocabulary\\nsize to any hard coded number, i.e. to get top N'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='most frequent words. The vocabulary size we got\\nfrom English sentences was 36838 and for Span-\\nish was 63220. Only token we added was End of\\nSentence and Start of Sentence Tokens. We used\\nSpanish as a source sentence and English as a tar-\\nget sentence. See Table 1.\\n5 Experiments\\nThe encoder and decoder of our model have 256\\nhidden units each. The encoder consist of for-\\nward and backward gated recurrent unit (GRU)\\neach having 256 hidden units. The decoder has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='a single forward gated recurrent unit (GRU), with\\n256 hidden dimensions, unlike 1000 hidden units,\\nas in Bahdanau et al. (2014) due to the hardware\\nlimitations. And we only trained the model for\\nSpanish to English translation.\\nWe used Adam optimizer to train the model, and\\ngradient update is computed with a batch size of\\n32 sentences.\\nWe initialized our weights with xavier (Glo-\\nrot) initializations, (Glorot and Bengio, 2010) with\\nUniform Distribution, U[−a,a], where\\na=\\n√\\n6\\nnin + nout'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='a=\\n√\\n6\\nnin + nout\\nwhere nin is the number of input neurons in the\\nweight tensor, and nout is the number of output\\nneurons in the weight tensor.\\nThe total number of trainable parameters were\\n43,564,519. We trained the model for roughly 20\\nhours. After our model was trained, we use greedy\\nsearch to predict the translation for the given input\\nsentence, that maximizes the conditional proba-\\nbility instead of using beam search as mentioned\\nin the paper Bahdanau et al. (2014) due to our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='unfamiliarity and technical difﬁculty dealing with\\nBeam search.\\nWe, then used BLEU (Papineni et al., 2002)\\nscore to evaluate how the model was working on\\nthe test data.\\n6 Results\\n6.1 Quantitative Results\\nWe trained our model on maximum length of 50\\nsentences, so any length smaller than or equal to\\n50 is used for the training, We trained our model\\nuntil the error on the validation data or the devel-\\nopment data stops decreasing, in order to avoid\\nthe problem of overﬁtting. We achieved a BLUE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='score of 25.76 on the test data and the error rate for\\nSpanish to English translation was 4.267 on our\\ntest data. According to Bahdanau et al. (2014), we\\ncan say that our model out perform the encoder\\ndecoder model, proposed by Cho et al. (2014b),\\nfor 50 sentences, where they got BLEU score of\\n17.82. Their performance drops when the length\\nof the sentence is increased (Cho et al., 2014b).\\nSo, the limitation of using ﬁxed length vector\\nin simple encoder decoder model in Cho et al.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='(2014b) work was the reason that it was under per-\\nforming with long sentences.\\nThis was our motivation to use the proposed ap-\\nproach by Bahdanau et al. (2014), where the per-\\nformance of the encoder-decoder with attention\\nshows no deterioration with sentence of length\\ngreater than 50 sentences. The result that we\\ngot which was 25.76 was quite close to the Bah-\\ndanau et al. (2014), where they got BLEU score of\\n26.75,training with 1000 encoder and decoder di-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='mensions, and training on corpus of 384M words.\\nThey were also able to achieve BLEU score of\\n28.45 when trained the data until the performance\\nof the validation stopped improving.\\n6.2 Qualitative Results\\nThe model proposed by Bahdanau et al. (2014)\\nprovides a way to investigate the soft alignment\\nbetween the translated sentence from the model\\nand the input sentence. The matrix given in Fig\\n3, each of the cells represent the weights αij of\\nthe annotation of the j-th source word for the i-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='th target word. This helps in visualizing and see\\nwhich word from the input sentence were con-\\nsidered more important for generating the target\\nword.\\nWe see that majority of the weights are con-\\ncentrated on the diagonal matrix, along with non-\\nmonotonic alignments. The non-monotonic align-\\nments would be high for long sentences, since the\\nwords in long target sentence tends to have depen-\\ndence on more than one word in source sentence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='(a)\\n (b)\\n(c)\\n (d)\\nFigure 3: Alignments translated from Spanish to English by our model. The row represents the translated\\nsentence, English and the column represents the source sentence, Spanish. Each of the cells of the matrix\\nrepresents weights, αij, of the annotation of the j-th source word for the i-th translated word. (1:White,\\n0:Black)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Let us take an example from the test set, con-\\nsider the source sentence:\\nson los estados miembros de la zona del\\neuro los que no han cumplido , en espe-\\ncialla republica federal de alemania que\\nse niega a mantener su promesa.\\nAnd its translation by our model is :\\nthat it is the member states of the euro\\narea which have not complied honour,\\nparticularly the federal republic of ger-\\nmany that refuses to keep their promise\\nto sustain their promise.\\nAnd the reference is:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='it is the member states of the euro area\\nthat have not delivered - and particu-\\nlarly the federal republic of germany,\\nwhich is refusing to keep its promise.\\nWe can observe from our translated sentence,\\nthe model generates ”have not complied ”, instead\\nof ”have not delivered”(from reference sentence),\\nwhich has the same meaning. It tries to preserve\\nthe meaning of the whole sentence, and it does not\\nblindly takes the word from the reference, it tries\\nto generalize.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='to generalize.\\nRefer to Table 4 at the end of the paper for\\nmore translations from Spanish to English using\\nEncoder Decoder model with Attention Mecha-\\nnism.\\n7 Discussion\\nAfter trying to achieve the result similar to what\\nwas presented in the paper, we are satisﬁed with\\nour result though there is much that can be im-\\nproved. Limitation caused by the hardware held\\nus back from achieving better results. We used the\\nserver provided by the university and as a backup'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='used google colab for our work, so we had to be\\nwary of the maintenance schedule happening in\\nthe server and the limitation of 24 hrs of work-\\ntime on google colab, which otherwise could in-\\nterrupt while we were training our model. So,\\nto overcome these challenges we decided to use\\n1M sentences from each form Spanish and English\\ndataset, and reduce the parameters for encoder-\\ndecoder.\\nWe also faced problem with length of the vo-\\ncabulary size of English and Spanish sentences,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='where the vocabulary size of the English sen-\\ntence were the output dimension, and vocabulary\\nsize of Spanish sentence were input dimensions of\\nour model. This leads to increase in number of\\ntrainable parameters, the decoding complexity in-\\ncreases with number of target words (vocabulary\\nsize of the English), where this problem has been\\naddressed by Jean et al. (2015).\\n8 Conclusion\\nThe approach proposed by Cho et al. (2014b) was\\nto encode the whole sentence into ﬁxed length vec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='tor, and this becomes problematic when dealing\\nwith long sentences. We extend this basic encoder-\\ndecoder model by an Attention mechanism (Bah-\\ndanau et al., 2014), where the model searches for\\nthe input word computed from the encoder, which\\nbest align with the target word, when generat-\\ning each target word. This frees the model from\\nhaving to encode the source sentence into a ﬁxed\\nlength vector, only rely on the information rele-\\nvant for generating each target word. We com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='pared our model for Spanish to English translation\\nwith both of these approaches, and found that our\\nmodel works better than the encoder-decoder ap-\\nproach (Cho et al., 2014b), and have slightly lower\\nresults than the architecture with Attention mech-\\nanism, (Bahdanau et al., 2014). We also observed\\nthat the model tries to align the target word with\\nthe relevant word from the translated sentence.\\nIn the future work, there are several things we\\ncan try. We can train our model on much larger'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='dataset, with a better hardware and with differ-\\nent attention models. We can also focus on how\\nto handle the stop words, punctuation, as we can\\nsee on Table 2 and Table 3, which accounts for\\nthe highest word count, and also to handle the un-\\nknown words, which does not appear in the train-\\ning data, but in the test data.\\nReferences\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\\nBengio. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint\\narXiv:1409.0473, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Kyunghyun Cho, B. V . Merrienboer, Dzmitry\\nBahdanau, and Yoshua Bengio. On the\\nproperties of neural machine translation: En-\\ncoder–decoder approaches. In SSST@EMNLP,\\n2014a.\\nKyunghyun Cho, B. V . Merrienboer, C ¸ aglar\\nG¨ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='Holger Schwenk, and Yoshua Bengio. Learn-\\ning phrase representations using rnn en-\\ncoder–decoder for statistical machine transla-\\ntion. In EMNLP, 2014b.\\nJ. Chung, C ¸ aglar G¨ulc ¸ehre, Kyunghyun Cho, and\\nYoshua Bengio. Empirical evaluation of gated\\nrecurrent neural networks on sequence model-\\ning. ArXiv, abs/1412.3555, 2014.\\nXavier Glorot and Yoshua Bengio. Understanding\\nthe difﬁculty of training deep feedforward neu-\\nral networks. In AISTATS, 2010.\\nS. Hochreiter. The vanishing gradient problem'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='during learning recurrent neural nets and prob-\\nlem solutions. Int. J. Uncertain. Fuzziness\\nKnowl. Based Syst., 6:107–116, 1998.\\nS. Hochreiter and J. Schmidhuber. Long short-\\nterm memory. Neural Computation, 9:1735–\\n1780, 1997.\\nS´ebastien Jean, Kyunghyun Cho, R. Memisevic,\\nand Yoshua Bengio. On using very large tar-\\nget vocabulary for neural machine translation.\\nArXiv, abs/1412.2007, 2015.\\nMarcin Junczys-Dowmunt, Tomasz Dwojak, and\\nHieu Hoang. Is neural machine translation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='ready for deployment? a case study on 30 trans-\\nlation directions. 01 2016.\\nPhilipp Koehn. Europarl: A parallel corpus for\\nstatistical machine translation. In MTSUMMIT,\\n2005.\\nPhilipp Koehn, Franz Josef Och, and Daniel\\nMarcu. Statistical phrase-based translation.\\nIn Proceedings of the 2003 Conference of the\\nNorth American Chapter of the Association\\nfor Computational Linguistics on Human Lan-\\nguage Technology - Volume 1, NAACL ’03,\\npage 48–54, USA, 2003. Association for Com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='putational Linguistics. doi: 10.3115/1073445.\\n1073462. URL https://doi.org/10.\\n3115/1073445.1073462.\\nThang Luong, Hieu Pham, and Christopher D.\\nManning. Effective approaches to attention-\\nbased neural machine translation. In EMNLP,\\n2015.\\nKishore Papineni, S. Roukos, T. Ward, and Wei-\\nJing Zhu. Bleu: a method for automatic evalua-\\ntion of machine translation. In ACL, 2002.\\nR. Rana. Gated recurrent unit (gru) for emo-\\ntion classiﬁcation from noisy speech. ArXiv,\\nabs/1612.07778, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='M. Schuster and K. Paliwal. Bidirectional recur-\\nrent neural networks. IEEE Trans. Signal Pro-\\ncess., 45:2673–2681, 1997.\\nXuewen Shi, Heyan Huang, Ping Jian, and Yi-Kun\\nTang. Improving neural machine translation\\nwith sentence alignment learning. Neurocom-\\nputing, 420:15–26, 2021. ISSN 0925-2312. doi:\\nhttps://doi.org/10.1016/j.neucom.2020.05.104.\\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le.\\nSequence to sequence learning with neural net-\\nworks. In NIPS, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='Shuangzhi Wu, Dongdong Zhang, Zhirui Zhang,\\nNan Yang, Mu Li, and M. Zhou. Dependency-\\nto-dependency neural machine translation.\\nIEEE/ACM Transactions on Audio, Speech, and\\nLanguage Processing, 26:2132–2141, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Source se˜nor presidente , se ˜nor presidente en ejercicio del consejo , se ˜nor presidente de la\\ncomisi´on , se˜nor´ıas , me gustar´ıa hacer tres breves comentarios .\\nReference mr president , mr president - in - ofﬁce of the council , mr president of the com-\\nmission , ladies and gentlemen , i would just like to make three brief comments\\n.\\nOur Model president mr president , mr president - in - ofﬁce of the council , mr president of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='the commission , ladies and gentlemen , i would like to make three brief comments\\nbrief comments .\\nGoogle Translate Mr. Chairman, Mr. Chairman-in-Ofﬁce of the Council, Mr. Chairman of the Com-\\nmittee, ladies and gentlemen, I would like to make three brief comments.\\nSource como los estados miembros , la comisi ´on procura promover el estado de derecho ,\\nsin el cual los derechos humanos no obtendr ´an reconocimiento en ning´un territorio\\n.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='.\\nReference the commission is involved , as are member states , in the promotion of the rule of\\nlaw , without which human rights can not operate in any territory .\\nOur Model that as the member states , the commission intends to promote the rule of law ,\\nwithout human rights will not not be any recognition in any territory in any territory\\n.\\nGoogle Translate Like the member states, the commission seeks to promote the rule of law, without'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='which human rights will not be recognized in any territory.\\nSource por escrito . - he votado a favor del informe de la se ˜nora fraga , que permite a\\ngroenlandia exportar productos pesqueros a la ue a pesar de no ser miembro .\\nReference in writing . - i voted in favour of ms fraga ’s report , which allows greenland to\\nexport ﬁshery products to the eu despite not being a member .\\nOur Model in writing . - i voted in favour of mrs fraga est´evez ’s report , which allows greenland'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='export export to export to the eu despite despite not being member .\\nGoogle Translate written . - i voted in favor of the report by mrs fraga, which allows greenland to\\nexport ﬁshery products to the eu despite not being a member.\\nSource ( pl ) se ˜nor presidente , me gustar ´ıa una vez m´as manifestar mi satisfacci ´on por la\\nimportancia que la comunidad conﬁere a la necesidad de innovaci´on en europa .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Reference ( pl ) mr president , i would like once again to express my pleasure at the importance\\nthat the community attaches to the need for innovation in europe .\\nOur Model that ( pl ) mr president , i would once again like to express my satisfaction satis-\\nfaction that the community attaches to the community to the need for innovation in\\neurope .\\nGoogle Translate (pl) mr president, i would like once again to express my satisfaction with the impor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='tance that the community attaches to the need for innovation in europe.\\nSource son los estados miembros de la zona del euro los que no han cumplido , en especial\\nla rep´ublica federal de alemania que se niega a mantener su promesa .\\nReference it is the member states of the euro area that have not delivered - and particularly the\\nfederal republic of germany , which is refusing to keep its promise .\\nOur Model that it is the member states of the euro area which have not complied honour ,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-11-05T20:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2021-11-05T20:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:355917108_AS:1086893412356097@1636146991241', 'source': '../assets/MachineTranslationwithAttention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='particularly the federal republic of germany that refuses to keep their promise to\\nsustain their promise .\\nGoogle Translate it is the eurozone member states that have not delivered, especially the federal re-\\npublic of germany which refuses to keep its promise.\\nTable 4: Source and Reference form the test data, with translated sentence from our model along with\\nGoogle translation (as of 16 August 2021)\\nView publication stats')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = text_splitter.split_documents(data)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b1335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
