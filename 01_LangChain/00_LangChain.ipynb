{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "![LangChain](https://miro.medium.com/v2/resize:fit:1358/1*Xyu6-ddpKUJFEg5e1vrFew.png)\n",
    "\n",
    "**LangChain** is a powerful open-source framework designed to streamline the development of applications leveraging large language models (LLMs). It provides a comprehensive set of tools and abstractions to seamlessly integrate LLMs with diverse data sources, APIs, and external systems, enabling the creation of sophisticated and interactive applications. LangChain focuses on:\n",
    "\n",
    "- **Connecting LLMs to External Tools:** It enables models to interact with APIs, databases, and other data sources, facilitating the retrieval and utilization of real-time information.\n",
    "\n",
    "- **Managing Conversational Context:** The framework excels in maintaining context and managing state across complex interactions, ensuring coherent and contextually relevant conversations.\n",
    "\n",
    "- **Facilitating Custom Logic:** Developers can implement custom logic and workflows, tailoring the behavior of LLMs to meet specific requirements and use cases.\n",
    "\n",
    "- **Accessibility to Various Models:** LangChain supports integration with different LLMs through APIs, including those from HuggingFace and other model providers, while also providing access to private data sources and third-party APIs.\n",
    "\n",
    "Overall, LangChain enhances the capabilities of LLMs by bridging them with external tools and data sources, making it easier to build dynamic and context-aware applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Getting Started\n",
    "\n",
    "- __Activate virtual env (optional):__ To activate the virtual environment enter this your terminal:\n",
    "\n",
    "```bash\n",
    "      source openai-env/bin/activate\n",
    "```\n",
    "\n",
    "- __Install LangChain:__ To install __langchain__, __langchain community__ and __langgraph__, enter the following command on your terminal:\n",
    "\n",
    "```bash\n",
    "      pip3 install langchain langchain-community langgraph\n",
    "```\n",
    "\n",
    "> [**Installation Reference Docs**](https://python.langchain.com/v0.1/docs/get_started/installation/)\n",
    "\n",
    "#### What we are going to learn?\n",
    "\n",
    "<div style=\"align:center\">\n",
    "\n",
    "![LangChain Usage Applications](https://media.licdn.com/dms/image/D5612AQGHBoBdQAJF_g/article-cover_image-shrink_720_1280/0/1682676410676?e=2147483647&v=beta&t=oih8lN6UMrC8cHDgMdQmIfls9zkmUPNZSAlzkSCJUiA)\n",
    "</div>\n",
    "\n",
    "| **Feature**                  | **Description**                                                                                                                                                         |\n",
    "|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Using OpenAI via LangChain**  | Learn how to integrate OpenAI's powerful models into your applications using LangChain, which streamlines interactions and enhances functionality through a unified framework. |\n",
    "| **Prompt Templating**           | Discover how to create and manage reusable prompt templates with LangChain, ensuring consistency and efficiency in generating model responses across various scenarios. |\n",
    "| **Chains**                     | Explore how to build and use chains in LangChain, which allow for the sequential execution of tasks and processing steps, facilitating complex workflows with OpenAI models. |\n",
    "| **Agents**                     | Understand how agents in LangChain can be employed to handle dynamic interactions and adapt to different contexts, leveraging OpenAI models to execute complex decision-making tasks. |\n",
    "| **Memory**                    | Learn how to implement memory in LangChain to retain context and manage state across interactions, allowing for more coherent and contextually aware conversations with OpenAI models. |\n",
    "| **Document Loader**           | Learn how to use LangChain's document loader to ingest, process, and manage large volumes of documents, enabling effective querying and interaction with text-based data. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. LangChain and OpenAI\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://go-skill-icons.vercel.app/api/icons?i=langchain,chatgpt\" alt=\"GenAI\"  height=\"55\"/>\n",
    "  </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os \n",
    "import openai \n",
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Load environment variables form .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables and setup OpenAI client\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_key\n",
    "\n",
    "# Initiate LangChain OpenAI Client\n",
    "# NOTE: The way of setting up OpenAI API client is different on langchain \n",
    "client = OpenAI(openai_api_key=openai_key)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prompt Methods\n",
    "\n",
    "#### 1.1.1 Zero-Shot Prompt\n",
    "\n",
    "A **zero-shot prompt** is a type of prompt where you ask a language model to perform a task or answer a question without providing any specific examples or prior training for that particular task. The model is expected to generate a response based on its general understanding and pre-existing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"\"\"\n",
    "Can you tell me how many human astronauts have landed on the moon? \n",
    "Can you give the names of those astronauts?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs of 2021, a total of 24 human astronauts have landed on the moon. \\nThe names of those astronauts are:\\n\\n1. Neil Armstrong\\n2. Buzz Aldrin\\n3. Charles Conrad\\n4. Alan Bean\\n5. Alan Shepard\\n6. Edgar Mitchell\\n7. David Scott\\n8. James Irwin\\n9. John Young\\n10. Charles Duke\\n11. Eugene Cernan\\n12. Harrison Schmitt\\n13. Michael Collins\\n14. Richard Gordon\\n15. Stuart Roosa\\n16. Alfred Worden\\n17. Pete Conrad\\n18. Al Bean\\n19. Alan L. Bean\\n20. Gene Cernan\\n21. Charles M. Duke Jr.\\n22. Edgar D. Mitchell\\n23. James B. Irwin\\n24. John W. Young'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2021, a total of 12 human astronauts have landed on the moon. \n",
      "\n",
      "The names of those astronauts are:\n",
      "\n",
      "1. Neil Armstrong \n",
      "2. Buzz Aldrin \n",
      "3. Pete Conrad \n",
      "4. Alan Bean \n",
      "5. Alan Shepard \n",
      "6. Edgar Mitchell \n",
      "7. David Scott \n",
      "8. James Irwin \n",
      "9. John Young \n",
      "10. Charles Duke \n",
      "11. Eugene Cernan \n",
      "12. Harrison Schmitt\n"
     ]
    }
   ],
   "source": [
    "print(client.invoke(zero_shot_prompt).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Few-Shot Prompt\n",
    "\n",
    "A **few-shot prompt** is a type of prompt where you provide the model with a few examples to guide it in generating the desired response. This method helps the model understand the context or format of the response by showing it examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "Here are some examples of questions and answers:\n",
    "\n",
    "1. Question: Who is the President of the United States?\n",
    "   Answer: Joe Biden is the President of the United States.\n",
    "\n",
    "2. Question: What is the capital of France?\n",
    "   Answer: The capital of France is Paris.\n",
    "\n",
    "Now, based on these examples, answer the following question:\n",
    "Question: How many human astronauts have landed on the moon? \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: A total of twelve human astronauts have landed on the moon.\n"
     ]
    }
   ],
   "source": [
    "print(client.invoke(few_shot_prompt).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Instruction-Based Prompt\n",
    "\n",
    "An **instruction-based prompt** is a type of prompt where you provide the model with a detailed instruction along with a question or a set of examples to guide it in generating the desired response. This method helps the model understand the context and format of the response by providing explicit instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"\"\"\n",
    "Provide a list of human astronauts who have landed on the moon. Include their names and the mission they were part of.\n",
    "\n",
    "List:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Neil Armstrong - Apollo 11\n",
      "2. Buzz Aldrin - Apollo 11\n",
      "3. Charles \"Pete\" Conrad - Apollo 12\n",
      "4. Alan Bean - Apollo 12\n",
      "5. Alan Shepard - Apollo 14\n",
      "6. Edgar Mitchell - Apollo 14\n",
      "7. David Scott - Apollo 15\n",
      "8. James Irwin - Apollo 15\n",
      "9. John Young - Apollo 16\n",
      "10. Charles Duke - Apollo 16\n",
      "11. Eugene Cernan - Apollo 17\n",
      "12. Harrison Schmitt - Apollo 17\n"
     ]
    }
   ],
   "source": [
    "print(client.invoke(instruction_prompt).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Contextual Prompt\n",
    "\n",
    "In a **contextual prompt**, you provide additional context or background information to help the model understand the task better. This can be especially useful when dealing with complex or domain-specific questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_prompt = \"\"\"\n",
    "In the history of space exploration, twelve astronauts have walked on the lunar surface. They were part of NASA's Apollo missions.\n",
    "\n",
    "Based on this context, please answer the following question:\n",
    "How many human astronauts have landed on the moon, each landing year and what are their names?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twelve human astronauts have landed on the moon, each in a different year. Their names are:\n",
      "\n",
      "1. Neil Armstrong (1969)\n",
      "2. Buzz Aldrin (1969)\n",
      "3. Pete Conrad (1969)\n",
      "4. Alan Bean (1969)\n",
      "5. Alan Shepard (1971)\n",
      "6. Edgar Mitchell (1971)\n",
      "7. David Scott (1971)\n",
      "8. James Irwin (1971)\n",
      "9. John Young (1972)\n",
      "10. Charles Duke (1972)\n",
      "11. Gene Cernan (1972)\n",
      "12. Harrison Schmitt (1972)\n"
     ]
    }
   ],
   "source": [
    "print(client.invoke(contextual_prompt).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5 Conversational Prompt\n",
    "\n",
    "In a **conversational prompt**, you structure the input as a dialogue to simulate a more natural interaction. This method can be useful for applications that require back-and-forth communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_prompt = \"\"\"\n",
    "User: How many human astronauts have landed on the moon?\n",
    "Assistant: Sure why not. There have been twelve astronauts who have walked on the lunar surface.\n",
    "\n",
    "User: Can you give me their names and through which rocket they landed on?\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! The astronauts are:\n",
      "\n",
      "1. Neil Armstrong - Apollo 11\n",
      "2. Buzz Aldrin - Apollo 11\n",
      "3. Pete Conrad - Apollo 12\n",
      "4. Alan Bean - Apollo 12\n",
      "5. Alan Shepard - Apollo 14\n",
      "6. Edgar Mitchell - Apollo 14\n",
      "7. David Scott - Apollo 15\n",
      "8. James Irwin - Apollo 15\n",
      "9. John Young - Apollo 16\n",
      "10. Charles Duke - Apollo 16\n",
      "11. Eugene Cernan - Apollo 17\n",
      "12. Harrison Schmitt - Apollo 17\n",
      "\n",
      "All of these astronauts landed on the moon through the Apollo spacecraft.\n"
     ]
    }
   ],
   "source": [
    "print(client.invoke(conversational_prompt).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.6 Comparative Prompt\n",
    "\n",
    "In a **comparative prompt**, you ask the model to compare or contrast two or more items or concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_prompt = \"\"\"\n",
    "Compare the Apollo 11 mission to the Apollo 12 mission. Include 2 important details in short such as the astronauts involved and key achievements.\n",
    "\n",
    "Apollo 11:\n",
    "Apollo 12:\n",
    "Apollo 13:\n",
    "Apollo 17:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apollo 11:\n",
      "- Astronauts: Neil Armstrong, Buzz Aldrin, Michael Collins\n",
      "- Key Achievement: Successfully landed on the moon and Neil Armstrong became the first person to walk on its surface.\n",
      "\n",
      "Apollo 12:\n",
      "- Astronauts: Pete Conrad, Alan Bean, Richard Gordon\n",
      "- Key Achievement: Conducted the second lunar landing and brought back the first color TV camera footage from the moon.\n",
      "\n",
      "Apollo 13:\n",
      "- Astronauts: Jim Lovell, Jack Swigert, Fred Haise\n",
      "- Key Achievement: Despite a near-disastrous malfunction, the crew safely returned to Earth in a demonstration of teamwork and ingenuity.\n",
      "\n",
      "Apollo 17:\n",
      "- Astronauts: Eugene Cernan, Ronald Evans, Harrison Schmitt\n",
      "- Key Achievement: Last manned mission to the moon, with the longest stay on the lunar surface and collection of the most lunar samples.\n"
     ]
    }
   ],
   "source": [
    "print(client.invoke(comparative_prompt).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.6 Chain of Thought(CoT) Prompt\n",
    "\n",
    "**Chain of Thought (CoT) prompting** is another effective method used to improve the performance of language models by guiding them through a reasoning process. This approach encourages the model to articulate its reasoning step-by-step, which can help in solving complex problems and generating more accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_thought_prompt = \"\"\"\n",
    "To determine how many human astronauts have landed on the moon and list their names, let's break it down into steps:\n",
    "\n",
    "1. Identify the total number of Apollo missions that landed on the moon.\n",
    "2. List the astronauts who participated in each Apollo mission.\n",
    "3. Aggregate the names of astronauts from all missions to find the total count and list them.\n",
    "\n",
    "Now, perform the above steps to answer the question:\n",
    "How many human astronauts have landed on the moon and what are their names?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Identify the total number of Apollo missions that landed on the moon:\n",
      "There were six Apollo missions that successfully landed on the moon:\n",
      "- Apollo 11: July 20, 1969\n",
      "- Apollo 12: November 19, 1969\n",
      "- Apollo 14: February 5, 1971\n",
      "- Apollo 15: July 30, 1971\n",
      "- Apollo 16: April 21, 1972\n",
      "- Apollo 17: December 11, 1972\n",
      "\n",
      "2. List the astronauts who participated in each Apollo mission:\n",
      "a. Apollo 11:\n",
      "- Neil Armstrong\n",
      "- Buzz Aldrin\n",
      "- Michael Collins\n",
      "\n",
      "b. Apollo 12:\n",
      "- Charles \"Pete\" Conrad Jr.\n",
      "- Alan L. Bean\n",
      "- Richard F. Gordon Jr.\n",
      "\n",
      "c. Apollo 14:\n",
      "- Alan B. Shepard Jr.\n",
      "- Edgar D. Mitchell\n",
      "- Stuart A. Roosa\n",
      "\n",
      "d. Apollo 15:\n",
      "- David R. Scott\n",
      "- James B. Irwin\n",
      "- Alfred M. Worden\n",
      "\n",
      "e. Apollo 16:\n",
      "- John W. Young\n",
      "- Charles M. Duke Jr.\n",
      "- Thomas K. Mattingly II\n",
      "\n",
      "f. Apollo 17:\n",
      "- Eugene A. C\n"
     ]
    }
   ],
   "source": [
    "print(client.invoke(chain_of_thought_prompt).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Prompt Templates\n",
    "\n",
    "In LangChain, a __Prompt Template__ is a powerful tool used to create and manage reusable templates for prompts sent to large language models (LLMs). These templates help standardize and streamline the process of generating prompts for various tasks or queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt template\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to create **Prompt Template**\n",
    "\n",
    "- __`PromptTemplate()`__\n",
    "\n",
    "- __`PromptTemplate.from_template()`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using `PromptTemplate()`\n",
    "prompt_temp_name = PromptTemplate(\n",
    "    input_variables=[\"planet\"],\n",
    "    template=\"How many moon does the planet {planet} have?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many moon does the planet Jupiter have?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the prompt template by assigning input_variable using `.format()`\n",
    "prompt1 = prompt_temp_name.format(planet=\"Jupiter\")\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many moon does the planet Saturn have?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 = prompt_temp_name.format(planet=\"Saturn\")\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template using `PromptTemplate.from_template()`\n",
    "prompt_temp_name1 = PromptTemplate.from_template(\"Who is the CEO of {company}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the CEO of Tesla?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt3 = prompt_temp_name1.format(company=\"Tesla\")\n",
    "prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the CEO of Apple?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt4 = prompt_temp_name1.format(company=\"Apple\")\n",
    "prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: How many moon does the planet Jupiter have?\n",
      "Response: Jupiter has 79 known moons.\n",
      "\n",
      "------------------------------\n",
      "Prompt 2: How many moon does the planet Saturn have?\n",
      "Response: Saturn has 82 known moons.\n",
      "\n",
      "------------------------------\n",
      "Prompt 3: Who is the CEO of Tesla?\n",
      "Response: Elon Musk is the CEO of Tesla.\n",
      "\n",
      "------------------------------\n",
      "Prompt 4: Who is the CEO of Apple?\n",
      "Response: As of 2021, the CEO of Apple is Tim Cook.\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate the response of all prompts\n",
    "prompt_lists = [prompt1, prompt2, prompt3, prompt4]\n",
    "\n",
    "for idx, prompt in enumerate(prompt_lists):\n",
    "    print(f\"Prompt {idx+1}: {prompt}\")\n",
    "    print(f\"Response: {client.invoke(prompt).strip()}\")\n",
    "    print(\"---\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Agents\n",
    "\n",
    "**Agents** are specialized components that handle specific tasks or interactions within a language model-based system. They can perform actions like \n",
    "- retrieving data or calling APIs, \n",
    "- manage complex queries, \n",
    "- facilitate communication between system components, and \n",
    "- maintain context throughout interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you tell me who won the Copa America and Euro Cup recently?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt5 = \"Can you tell me who won the Copa America and Euro Cup recently?\"\n",
    "prompt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Copa America was won by Brazil in 2019, while the Euro Cup was won by Italy in 2021.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke(prompt5).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook, created in **July 2024**, the response provided was **outdated**, referencing data from **2019** and **2021**. This is because the ChatGPT model is trained on past data, which results in it not being up-to-date.\n",
    "\n",
    "To extract **real-time information**, we use **SERP API** to call the **Google Search Engine** and extract information in real-time.\n",
    "\n",
    "<div style=\"align:center\">\n",
    "\n",
    "![SerpAPI](https://serpapi.com/assets/SerpApi-gentle-white-dc708ed0b09596cd08db74108b88af298123eeb384e872716b61d2a0ba1a1e03.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Getting Started with Serp API\n",
    "\n",
    "To get started with **SerpAPI**, follow these steps:\n",
    "\n",
    "1. **Create an Account**: Visit the [SerpAPI website](https://serpapi.com) and sign up for an account.\n",
    "\n",
    "2. **Get API Key**: Place the api key in `.env` under a variable name. \n",
    "\n",
    "3. **Install the Library**: In your terminal or command prompt, install the SerpAPI Python library by running the following command:\n",
    "\n",
    "   ```bash\n",
    "   pip install google-search-results\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables form .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables \n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "serp_key = os.getenv(\"SERP_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary agent tools\n",
    "from langchain.agents import AgentType, load_tools, create_react_agent\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Initiate LangChain OpenAI Client\n",
    "client = OpenAI()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_react_agent() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize agent tool with SerpAPI and initialize OpenAI model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tool \u001b[38;5;241m=\u001b[39m load_tools([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserpapi\u001b[39m\u001b[38;5;124m\"\u001b[39m], serpapi_api_key\u001b[38;5;241m=\u001b[39mserp_key)\n\u001b[0;32m----> 4\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_react_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt5\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m                                     \n",
      "\u001b[0;31mTypeError\u001b[0m: create_react_agent() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "# Initialize agent tool with SerpAPI and initialize OpenAI model\n",
    "tool = load_tools([\"serpapi\"], serpapi_api_key=serp_key)\n",
    "\n",
    "agent = create_react_agent(tools=tool, model=client,\n",
    "                           prompt=prompt5\n",
    ")                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt5\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m      5\u001b[0m }\n",
      "File \u001b[0;32m~/Desktop/LangChain-Generative-AI/env/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1263\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1262\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1263\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LangChain-Generative-AI/env/lib/python3.11/site-packages/langgraph/pregel/__init__.py:968\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;66;03m# handle exit\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    970\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout hitting a stop condition. You can increase the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit by setting the `recursion_limit` config key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m    974\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(read_channels(loop\u001b[38;5;241m.\u001b[39mchannels, output_keys))\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key."
     ]
    }
   ],
   "source": [
    "agent.invoke({\"messages\": [(\"human\", prompt5)]})\n",
    "{\n",
    "    \"input\": query,\n",
    "    \"output\": messages[\"messages\"][-1].content,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
